# ğŸš€ WAX Data Ingestion Pipeline â€“ Version Databricks Production

Ce projet est une solution complÃ¨te d'ingestion de donnÃ©es ZIP + Excel + CSV vers Delta Lake dans Unity Catalog, avec dÃ©clenchement automatisÃ© via API FastAPI, configuration dynamique par `spark.conf`, et portabilitÃ© totale entre environnements (dev, prod...).

---

## ğŸ“¦ Contenu du projet

```
projet_wax/
â”œâ”€â”€ api/                       # API FastAPI pour lancer les jobs Databricks
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ .env
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_pipeline.py        # Lancement principal
â”‚   â””â”€â”€ run_local.py           # Lancement pour test local (spark local mode)
â”œâ”€â”€ src/wax_pipeline/          # Modules Python
â”‚   â”œâ”€â”€ config.py              # Configuration basÃ©e sur spark.conf
â”‚   â”œâ”€â”€ ingestion.py           # Orchestration du pipeline
â”‚   â”œâ”€â”€ file_processor.py      # Chargement des fichiers
â”‚   â”œâ”€â”€ delta_manager.py       # Sauvegarde Delta / Unity Catalog
â”‚   â”œâ”€â”€ utils.py, validator.py, etc.
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_validator.py
â”œâ”€â”€ azure-pipelines.yml        # Pipeline CI/CD Azure DevOps
â””â”€â”€ requirements.txt           # DÃ©pendances globales
```

---

## âš™ï¸ PrÃ©requis

- Workspace **Databricks (Unity Catalog)**
- Un cluster compatible UC
- Fichiers dâ€™entrÃ©e dans `/Volumes/<catalog>/<schema>/<volume>/`
- Token personnel Databricks (pour lâ€™API)
- Job Databricks dÃ©jÃ  crÃ©Ã© (voir ci-dessous)

---

## ğŸ“ Structure des fichiers d'entrÃ©e attendus

- `site.zip` : archive contenant des fichiers `.csv`
- `site_config.xlsx` : fichier Excel contenant la configuration de mapping CSV â†’ tables Delta

---

## ğŸš€ Lancement via Databricks Jobs

### ğŸ¯ ParamÃ¨tres Ã  passer au job (via UI ou API)

```json
{
  "spark_conf": {
    "catalog_name": "dev_catalog",
    "schema_name": "bronze",
    "volume_name": "landing",
    "env": "dev",
    "zip_path": "/Volumes/dev_catalog/bronze/landing/site.zip",
    "excel_path": "/Volumes/dev_catalog/bronze/landing/site_config.xlsx"
  }
}
```

ğŸ§  Ces paramÃ¨tres sont rÃ©cupÃ©rÃ©s automatiquement via `spark.conf.get()` dans `config.py`.

---

## ğŸŒ DÃ©clenchement via API FastAPI

### â–¶ï¸ Lancer lâ€™API :

```bash
cd api
uvicorn app:app --reload
```

### ğŸ§ª Tester avec Postman :

- **MÃ©thode** : `POST`
- **URL** : `http://localhost:8000/launch-pipeline`
- **Body** :

```json
{
  "catalog": "dev_catalog",
  "schema": "bronze",
  "volume": "landing",
  "env": "dev",
  "zip_path": "/Volumes/dev_catalog/bronze/landing/site.zip",
  "excel_path": "/Volumes/dev_catalog/bronze/landing/site_config.xlsx"
}
```

Lâ€™API appelle ensuite le **Job Databricks** en lui passant les paramÃ¨tres `spark_conf`.

---

## âœ… FonctionnalitÃ©s clÃ©s

- ğŸ”„ Ingestion automatique de fichiers `.csv` contenus dans une archive `.zip`
- ğŸ§  Mapping via `site_config.xlsx`
- ğŸªµ Logging qualitÃ© + erreurs
- ğŸ§ª Validation : types, formats, valeurs obligatoires, doublons
- ğŸ”— Sauvegarde en Delta + Unity Catalog
- ğŸŒ PortabilitÃ© multi-environnement (dev, staging, prod)
- ğŸŒ DÃ©clenchement via API
- â˜ï¸ PrÃªt pour Azure DevOps

---

## ğŸ§ª Tests

Lancer les tests unitaires :

```bash
pytest tests/
```

---

## ğŸ› ï¸ Pipeline CI/CD (Azure DevOps)

Le fichier `azure-pipelines.yml` :

- Installe les dÃ©pendances
- Lance les tests
- DÃ©ploie et vÃ©rifie lâ€™API FastAPI

---

## ğŸ” Variables sensibles

CrÃ©er un fichier `.env` dans `api/` :

```env
DATABRICKS_TOKEN=xxxxx
DATABRICKS_INSTANCE=https://<instance>.cloud.databricks.com
DATABRICKS_JOB_ID=1234
```

---

## ğŸ§  Astuce : Surveiller un run via API

```bash
GET https://<instance>.cloud.databricks.com/api/2.1/jobs/runs/get?run_id=98765
Authorization: Bearer <your_token>
```

---

## ğŸ“« Contact

Ce projet a Ã©tÃ© conÃ§u pour Ãªtre modulaire, Ã©volutif et dÃ©ployable dans un contexte Data Engineering professionnel.  
Nâ€™hÃ©site pas Ã  lâ€™adapter Ã  tes besoins !

