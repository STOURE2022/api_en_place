# ğŸ‰ NOUVELLES FEATURES R1 - Documentation

## ğŸ“‹ Vue d'Ensemble

Ce document dÃ©taille les **5 nouvelles features prioritaires** implÃ©mentÃ©es pour la Release 1 (R1).

---

## âœ… 1. NON-ZIPPED FILE HANDLING

### ğŸ¯ Objectif

GÃ©rer les fichiers CSV/Parquet/JSON directs (non-ZIP) en plus des archives ZIP.

### ğŸ“– SpÃ©cification Gherkin

```gherkin
Scenario: Non-zipped file received
Given a file is received
And the file is not a zip archive
When the file is processed
Then the file is copied to the raw destination folder
```

### ğŸ”§ ImplÃ©mentation

**Fichier:** `file_handler.py`

**FonctionnalitÃ©s:**
- âœ… DÃ©tection automatique fichiers non-ZIP
- âœ… Support formats: .csv, .parquet, .json, .txt
- âœ… Copie vers `/extracted/<table_name>/`
- âœ… Organisation automatique par table
- âœ… Gestion erreurs

### ğŸ“ Utilisation

```python
from file_handler import FileHandler

# Initialiser
handler = FileHandler(spark, config)

# Lister fichiers non-ZIP
files = handler.list_non_zip_files()
print(f"Fichiers trouvÃ©s: {files}")

# Traiter tous les fichiers
result = handler.process_non_zip_files()

# RÃ©sultat
# {
#   "status": "SUCCESS",
#   "file_count": 5,
#   "failed_count": 0,
#   "results": [...]
# }
```

### âš™ï¸ Configuration

```json
{
  "non_zip_support_enabled": true
}
```

### ğŸ“Š Workflow

```
ğŸ“‚ input/
â”œâ”€â”€ customers.csv          â† Non-ZIP
â”œâ”€â”€ orders.zip             â† ZIP
â””â”€â”€ products.parquet       â† Non-ZIP
    â†“
ğŸ“‚ extracted/
â”œâ”€â”€ customers/
â”‚   â””â”€â”€ customers.csv      âœ… CopiÃ©
â”œâ”€â”€ orders/
â”‚   â”œâ”€â”€ order_001.csv      âœ… Extrait ZIP
â”‚   â””â”€â”€ order_002.csv      âœ… Extrait ZIP
â””â”€â”€ products/
    â””â”€â”€ products.parquet   âœ… CopiÃ©
```

---

## âœ… 2. DUPLICATE PREVENTION

### ğŸ¯ Objectif

EmpÃªcher le traitement d'un fichier dÃ©jÃ  ingÃ©rÃ©.

### ğŸ“– SpÃ©cification Gherkin

```gherkin
Scenario: Prevent processing of the same file
Given file name received and all filenames ingested
When file name exists in all filenames ingested
Then Reject the file, Raise error "000000002"
```

### ğŸ”§ ImplÃ©mentation

**Fichier:** `tracking_manager.py`

**FonctionnalitÃ©s:**
- âœ… Table de tracking: `wax_processed_files`
- âœ… VÃ©rification avant traitement
- âœ… Error code: `000000002`
- âœ… Historique complet

### ğŸ“ Utilisation

```python
from tracking_manager import TrackingManager

# Initialiser
tracking = TrackingManager(spark, config)

# VÃ©rifier duplicate
result = tracking.check_duplicate(
    table_name="customers",
    filename="customers_20251016.csv"
)

if result["is_duplicate"]:
    print(f"âŒ Erreur {result['error_code']}: {result['message']}")
    # File customers_20251016.csv already processed
else:
    # Traiter fichier...
    
    # Enregistrer aprÃ¨s traitement
    tracking.register_file(
        table_name="customers",
        filename="customers_20251016.csv",
        status="SUCCESS",
        row_count=1000
    )
```

### âš™ï¸ Configuration

```json
{
  "duplicate_check_enabled": true
}
```

### ğŸ“Š Table de Tracking

```sql
-- Structure: wax_processed_files
CREATE TABLE wax_processed_files (
    table_name STRING NOT NULL,
    filename STRING NOT NULL,
    file_date TIMESTAMP,
    processed_date TIMESTAMP NOT NULL,
    status STRING NOT NULL,  -- SUCCESS ou FAILED
    row_count STRING
)
```

### ğŸ” RequÃªtes Utiles

```python
# Afficher rÃ©sumÃ©
tracking.display_tracking_summary()

# Lister fichiers traitÃ©s
files = tracking.get_processed_files(table_name="customers")

# Nettoyer anciens
tracking.cleanup_old_records(days_to_keep=90)
```

---

## âœ… 3. FILE ORDER VALIDATION

### ğŸ¯ Objectif

VÃ©rifier que les fichiers arrivent dans l'ordre chronologique.

### ğŸ“– SpÃ©cification Gherkin

```gherkin
Scenario: Prevent processing of old file
Given date in filename and previous date
When date < previous date
Then Reject file, Raise error "000000003"
```

### ğŸ”§ ImplÃ©mentation

**Fichier:** `tracking_manager.py` (mÃªme classe)

**FonctionnalitÃ©s:**
- âœ… Extraction date depuis filename
- âœ… Comparaison avec derniÃ¨re date
- âœ… Error code: `000000003`
- âœ… Support patterns multiples

### ğŸ“ Utilisation

```python
# VÃ©rifier ordre
result = tracking.check_file_order(
    table_name="customers",
    filename="customers_20251015.csv"  # Date < derniÃ¨re
)

if not result["valid_order"]:
    print(f"âŒ Erreur {result['error_code']}: {result['message']}")
    # File date 2025-10-15 < last processed 2025-10-16
```

### âš™ï¸ Configuration

```json
{
  "file_order_check_enabled": true
}
```

### ğŸ”¤ Patterns Filename SupportÃ©s

```
âœ… filename_yyyymmdd_hhmmss.csv   â†’ 20251016_101112
âœ… filename_yyyymmdd.csv           â†’ 20251016
âœ… yyyymmdd_filename.csv           â†’ 20251016_filename
âœ… filename_yyyy-mm-dd.csv         â†’ 2025-10-16
âœ… filename_yyyy_mm_dd.csv         â†’ 2025_10_16
```

### ğŸ“Š Validation CombinÃ©e

```python
# Validation complÃ¨te (duplicate + ordre)
validation = tracking.validate_file(
    table_name="customers",
    filename="customers_20251016.csv"
)

if not validation["valid"]:
    for error in validation["errors"]:
        print(f"âŒ {error['type']}: {error['message']}")
        print(f"   Code: {error['error_code']}")
```

---

## âœ… 4. LAST VS ALL TABLES

### ğŸ¯ Objectif

CrÃ©er deux tables distinctes selon le mode d'ingestion :
- `<table>_last` : DerniÃ¨res donnÃ©es
- `<table>_all` : Historique complet

### ğŸ“– SpÃ©cification Gherkin

```gherkin
Scenario: FULL_SNAPSHOT mode
Given ingestion mode is "FULL_SNAPSHOT"
Then create or update "<last_table>" containing new file
And create or update "<all_table>" containing all files

Scenario: DELTA_FROM_FLOW mode
Given ingestion mode is "DELTA_FROM_FLOW"
Then create or update "<all_table>" ONLY (no last table)
```

### ğŸ”§ ImplÃ©mentation

**Fichier:** `ingestion_enhanced.py`

**FonctionnalitÃ©s:**
- âœ… Gestion automatique Last/All
- âœ… Noms personnalisables
- âœ… ConformitÃ© stricte aux specs
- âœ… 5 modes d'ingestion

### ğŸ“ Utilisation

```python
from ingestion_enhanced import IngestionManagerEnhanced

# Initialiser
ingestion = IngestionManagerEnhanced(spark, config, delta_manager)

# Mode FULL_SNAPSHOT: crÃ©e Last + All
stats = ingestion.apply_ingestion_mode(
    df_raw=df,
    column_defs=column_defs,
    table_name="customers",
    ingestion_mode="FULL_SNAPSHOT",
    file_name_received="customers.csv"
)

# RÃ©sultat:
# â€¢ customers_last âœ… crÃ©Ã© (overwrite)
# â€¢ customers_all  âœ… crÃ©Ã© (append)

# Mode DELTA_FROM_FLOW: All uniquement
stats = ingestion.apply_ingestion_mode(
    df_raw=df,
    column_defs=column_defs,
    table_name="orders",
    ingestion_mode="DELTA_FROM_FLOW",
    file_name_received="orders.csv"
)

# RÃ©sultat:
# â€¢ orders_last âŒ NON crÃ©Ã©
# â€¢ orders_all  âœ… crÃ©Ã© (append)
```

### ğŸ“Š Modes d'Ingestion

| Mode | Last Table | All Table | Description |
|------|------------|-----------|-------------|
| **FULL_SNAPSHOT** | âœ… Overwrite | âœ… Append | Snapshot complet |
| **DELTA_FROM_FLOW** | âŒ Pas crÃ©Ã© | âœ… Append | Delta uniquement |
| **DELTA_FROM_NON_HISTORIZED** | âœ… Merge | âœ… Append | Merge sur clÃ©s |
| **DELTA_FROM_HISTORIZED** | âœ… Append | âœ… Append | Historique SCD2 |
| **FULL_KEY_REPLACE** | âœ… Delete+Insert | âœ… Append | Remplacement clÃ©s |

### âš™ï¸ Configuration Excel

```
File-Table Sheet:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Delta Table Name    â”‚ customers         â”‚
â”‚ Last Table Name     â”‚ customers_latest  â”‚  â† PersonnalisÃ© (optionnel)
â”‚ Ingestion mode      â”‚ FULL_SNAPSHOT     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Si Last Table Name vide â†’ <table>_last par dÃ©faut
```

### ğŸ” VÃ©rification

```python
# VÃ©rifier tables crÃ©Ã©es
spark.sql("SHOW TABLES IN catalog.schema").show()

# customers_all   âœ…
# customers_last  âœ…

# Compter lignes
all_count = spark.table("customers_all").count()
last_count = spark.table("customers_last").count()

print(f"All: {all_count}, Last: {last_count}")
```

---

## âœ… 5. INVALID LINES TABLE

### ğŸ¯ Objectif

Sauvegarder les lignes rejetÃ©es dans une table dÃ©diÃ©e pour audit et debug.

### ğŸ“– SpÃ©cification Gherkin

```gherkin
Rule: Invalid Lines Generate Rule
When Invalid lines generated is true
Then keep all invalid lines in a Table
```

### ğŸ”§ ImplÃ©mentation

**Fichier:** `invalid_lines_manager.py`

**FonctionnalitÃ©s:**
- âœ… Table: `<table>_invalid_lines`
- âœ… MÃ©tadonnÃ©es complÃ¨tes
- âœ… Raisons de rejet
- âœ… Nettoyage automatique

### ğŸ“ Utilisation

```python
from invalid_lines_manager import InvalidLinesManager

# Initialiser
invalid_mgr = InvalidLinesManager(spark, config)

# Sauvegarder lignes invalides
invalid_mgr.save_invalid_lines(
    df=invalid_df,
    table_name="customers",
    filename="customers_20251016.csv",
    rejection_reason="Validation failed",
    error_type="NULL_VALUE",
    error_details="customer_id is NULL"
)

# Afficher rÃ©sumÃ©
invalid_mgr.get_invalid_lines_summary("customers")

# Compter
count = invalid_mgr.get_invalid_lines_count("customers")
print(f"Total lignes invalides: {count}")
```

### âš™ï¸ Configuration

```json
{
  "invalid_lines_generated": true,
  "invalid_lines_cleanup_days": 30
}
```

### ğŸ“Š Structure Table

```sql
-- Structure: <table>_invalid_lines
CREATE TABLE customers_invalid_lines (
    invalid_line_id BIGINT NOT NULL,
    filename STRING NOT NULL,
    rejection_date TIMESTAMP NOT NULL,
    rejection_reason STRING NOT NULL,
    error_type STRING NOT NULL,
    error_details STRING,
    line_number BIGINT,
    raw_data STRING,
    -- + toutes les colonnes de la table source
    customer_id INT,
    name STRING,
    email STRING,
    ...
)
```

### ğŸ” RequÃªtes Utiles

```sql
-- Lignes invalides par type d'erreur
SELECT error_type, COUNT(*) as count
FROM customers_invalid_lines
GROUP BY error_type
ORDER BY count DESC;

-- DerniÃ¨res erreurs
SELECT filename, rejection_reason, rejection_date
FROM customers_invalid_lines
ORDER BY rejection_date DESC
LIMIT 10;

-- Erreurs par fichier
SELECT filename, COUNT(*) as errors
FROM customers_invalid_lines
GROUP BY filename
ORDER BY errors DESC;
```

### ğŸ§¹ Nettoyage

```python
# Nettoyer anciennes lignes (> 30 jours)
invalid_mgr.cleanup_invalid_lines(
    table_name="customers",
    days_to_keep=30
)
```

---

## âœ… BONUS: FAIL FAST OPTION

### ğŸ¯ Objectif

ArrÃªter l'ingestion immÃ©diatement si le seuil d'erreurs est atteint.

### ğŸ“– SpÃ©cification Gherkin

```gherkin
Scenario: Stop ingestion when threshold reached
Given error percentage threshold
When percentage of errors >= threshold
Then stop ingestion immediately
And Reject file, Raise error "000000007"
```

### ğŸ”§ ImplÃ©mentation

**Fichier:** `column_processor.py` (intÃ©grÃ©)

### âš™ï¸ Configuration

```json
{
  "fail_fast_enabled": true,
  "fail_fast_threshold": 10
}
```

### ğŸ“ Utilisation

```python
# Dans main_enhanced.py
api_params = {
    "fail_fast_enabled": True,
    "fail_fast_threshold": 10  # 10% d'erreurs max
}

pipeline = WAXPipelineEnhanced(spark, config, api_params)
```

### ğŸ“Š Comportement

```
Lignes lues : 100
Lignes erreur: 11
Seuil       : 10%

â†’ 11/100 = 11% >= 10%
â†’ âŒ ARRÃŠT IMMÃ‰DIAT
â†’ Error code: 000000007
```

---

## ğŸ“‹ CONFIGURATION COMPLÃˆTE

### config_params_enhanced.json

```json
{
  "dev": {
    "catalog": "abu_catalog",
    "schema_files": "databricksassetbundletest",
    "volume": "externalvolumetes",
    "schema_tables": "gdp_poc_dev",
    "env": "dev",
    "version": "v2.0.0",
    
    "fail_fast_enabled": true,
    "fail_fast_threshold": 10,
    "invalid_lines_generated": true,
    "duplicate_check_enabled": true,
    "file_order_check_enabled": true,
    "non_zip_support_enabled": true,
    "delete_zip_after_extract": true,
    "input_header_mode": "empty",
    "ict_default": 10,
    "rlt_default": 10,
    "partitioning_default": "yyyy/mm/dd",
    "tracking_cleanup_days": 90,
    "invalid_lines_cleanup_days": 30
  }
}
```

---

## ğŸš€ PIPELINE COMPLET

### main_enhanced.py

```python
from main_enhanced import WAXPipelineEnhanced

# Initialiser
pipeline = WAXPipelineEnhanced(spark, config, api_params)

# ExÃ©cuter
stats = pipeline.run(config_excel="/path/to/config.xlsx")

# Workflow complet:
# 1. âœ… Traitement fichiers non-ZIP
# 2. âœ… Extraction ZIP
# 3. âœ… Validation tracking (duplicate + ordre)
# 4. âœ… Auto Loader
# 5. âœ… Validation donnÃ©es
# 6. âœ… Sauvegarde lignes invalides
# 7. âœ… Ingestion (Last + All selon mode)
# 8. âœ… Logs et mÃ©triques
```

---

## ğŸ“Š NOUVEAUX FICHIERS

```
src/
â”œâ”€â”€ tracking_manager.py           âœ… Duplicate + File order
â”œâ”€â”€ file_handler.py               âœ… Non-ZIP files
â”œâ”€â”€ invalid_lines_manager.py      âœ… Invalid lines table
â”œâ”€â”€ ingestion_enhanced.py         âœ… Last/All tables
â””â”€â”€ main_enhanced.py              âœ… Pipeline complet

config_api/
â””â”€â”€ config_params_enhanced.json   âœ… Nouveaux paramÃ¨tres
```

---

## âœ… CHECKLIST VALIDATION

### Tests Unitaires

```bash
# Test tracking
python tracking_manager.py

# Test file handler
python file_handler.py

# Test invalid lines
python invalid_lines_manager.py

# Test ingestion
python ingestion_enhanced.py
```

### Tests d'IntÃ©gration

```bash
# Pipeline complet
python main_enhanced.py
```

### Validation Features

- [ ] Non-ZIP files copiÃ©s correctement
- [ ] Duplicate dÃ©tectÃ© (error 000000002)
- [ ] File order validÃ© (error 000000003)
- [ ] Tables Last/All crÃ©Ã©es selon mode
- [ ] Lignes invalides sauvegardÃ©es
- [ ] Fail fast fonctionne
- [ ] Logs complets

---

## ğŸ“š DOCUMENTATION ADDITIONNELLE

- `FEATURES_COVERAGE_ANALYSIS.md` : Analyse complÃ¨te couverture
- `SPECIFICATIONS_MAPPING.md` : Mapping specs â†’ code
- `README.md` : Documentation gÃ©nÃ©rale

---

## ğŸ‰ RÃ‰SUMÃ‰

### Avant (v1.0.0)

âŒ Seulement ZIP supportÃ©s
âŒ Pas de duplicate prevention
âŒ Pas de file order validation
âŒ Une seule table crÃ©Ã©e
âŒ Pas de table invalid lines
âŒ Fail fast non configurable

### AprÃ¨s (v2.0.0)

âœ… Non-ZIP + ZIP supportÃ©s
âœ… Duplicate prevention (000000002)
âœ… File order validation (000000003)
âœ… Last + All tables selon mode
âœ… Table invalid lines complÃ¨te
âœ… Fail fast configurable

---

**Version:** 2.0.0  
**Date:** Octobre 2025  
**Statut:** âœ… Production Ready
