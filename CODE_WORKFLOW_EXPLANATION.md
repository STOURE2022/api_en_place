# ğŸ”„ FONCTIONNEMENT GÃ‰NÃ‰RAL DU CODE WAX v2.0.0

## ğŸ“‹ Vue d'Ensemble

Le pipeline WAX est un systÃ¨me d'ingestion de donnÃ©es qui traite des fichiers (ZIP ou directs) et les charge dans des tables Delta Lake avec validation et traÃ§abilitÃ© complÃ¨tes.

---

## ğŸ¯ WORKFLOW GLOBAL - Vue SimplifiÃ©e

```
ğŸ“‚ Fichiers Arrivants
     â†“
[1] DÃ©tection Type (ZIP vs Non-ZIP)
     â†“
[2] Extraction/Copie â†’ /extracted/
     â†“
[3] Validation Tracking (Duplicate + Ordre)
     â†“
[4] Lecture DonnÃ©es (Auto Loader)
     â†“
[5] Validation DonnÃ©es (Colonnes, Types, Nulls)
     â†“
[6] Traitement Colonnes (Transformations)
     â†“
[7] Gestion Erreurs (Invalid Lines)
     â†“
[8] Ingestion Delta Lake (Last + All)
     â†“
[9] Logs & MÃ©triques
     â†“
âœ… Tables Delta PrÃªtes
```

---

## ğŸš€ WORKFLOW DÃ‰TAILLÃ‰ - Ã‰tape par Ã‰tape

### ğŸ“ PHASE 1: ARRIVÃ‰E FICHIERS

```
/Volumes/catalog/schema/volume/input/zip/
â”œâ”€â”€ customers.csv              â† Fichier direct (non-ZIP)
â”œâ”€â”€ orders.zip                 â† Archive ZIP
â””â”€â”€ products_20251016.csv      â† Fichier avec date
```

**Ce qui se passe:**
- Les fichiers sont dÃ©posÃ©s dans `/input/zip/`
- Le pipeline dÃ©tecte automatiquement leur prÃ©sence
- DÃ©marre le traitement

---

### ğŸ” PHASE 2: DÃ‰TECTION & EXTRACTION

#### Module: `file_handler.py` + `unzip_module.py`

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FICHIER ARRIVÃ‰                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â†“
         [Est-ce un ZIP?]
              â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
       â”‚             â”‚
     OUI            NON
       â”‚             â”‚
       â†“             â†“
  [unzip_module]  [file_handler]
  Extraction      Copie directe
  dans ZIP        
       â”‚             â”‚
       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
              â†“
     /extracted/<table>/
     â”œâ”€â”€ file1.csv
     â”œâ”€â”€ file2.csv
     â””â”€â”€ file3.csv
```

**Exemple concret:**

```python
# customers.csv (non-ZIP)
file_handler.process_non_zip_files()
â†’ Copie vers: /extracted/customers/customers.csv

# orders.zip (ZIP contenant order_001.csv, order_002.csv)
unzip_module.process_zips()
â†’ Extrait vers: /extracted/orders/order_001.csv
â†’ Extrait vers: /extracted/orders/order_002.csv
â†’ Supprime: orders.zip (si configured)
```

---

### ğŸ›¡ï¸ PHASE 3: VALIDATION TRACKING

#### Module: `tracking_manager.py`

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FICHIER EXTRAIT                        â”‚
â”‚  customers_20251016.csv                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â†“
     [CHECK DUPLICATE]
     Table: wax_processed_files
              â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
       â”‚             â”‚
   EXISTE         NOUVEAU
       â”‚             â”‚
       â†“             â†“
   âŒ REJECT      âœ… CONTINUE
   Error 000000002    â”‚
                      â†“
              [CHECK FILE ORDER]
              Date dans filename vs
              derniÃ¨re date traitÃ©e
                      â”‚
               â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
               â”‚             â”‚
          DATE < LAST    DATE â‰¥ LAST
               â”‚             â”‚
               â†“             â†“
           âŒ REJECT      âœ… CONTINUE
           Error 000000003
```

**Exemple concret:**

```python
# tracking_manager valide le fichier
validation = tracking_manager.validate_file(
    table_name="customers",
    filename="customers_20251016.csv"
)

if not validation["valid"]:
    print("Fichier rejetÃ©:")
    for error in validation["errors"]:
        print(f"  - {error['error_code']}: {error['message']}")
    # STOP - ne pas traiter ce fichier
else:
    # OK - continuer le traitement
    pass
```

---

### ğŸ“– PHASE 4: LECTURE DONNÃ‰ES

#### Module: `autoloader_module.py`

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FICHIER VALIDÃ‰                         â”‚
â”‚  /extracted/customers/                  â”‚
â”‚    customers_20251016.csv               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â†“
      [AUTO LOADER]
      - Lecture CSV/Parquet/JSON
      - InfÃ©rence schÃ©ma (optionnel)
      - Parsing selon dÃ©limiteur
              â”‚
              â†“
      [DataFrame PySpark]
      +-------------+--------+
      | customer_id | name   |
      +-------------+--------+
      | 1           | Alice  |
      | 2           | Bob    |
      | NULL        | Charlie|  â† ProblÃ¨me potentiel
      +-------------+--------+
```

**Exemple concret:**

```python
# Auto Loader charge le fichier
df = autoloader.load_file(
    file_path="/extracted/customers/customers_20251016.csv",
    file_format="csv",
    delimiter=","
)

print(f"Lignes lues: {df.count()}")
# Lignes lues: 1000
```

---

### âœ… PHASE 5: VALIDATION DONNÃ‰ES

#### Module: `validator.py`

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DataFrame ChargÃ©                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â†“
   [VALIDATION 1: Colonnes]
   Toutes colonnes prÃ©sentes?
   Types corrects?
              â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
       â”‚             â”‚
     NON            OUI
       â”‚             â”‚
       â†“             â†“
   âŒ REJECT      [VALIDATION 2: Nullables]
   Error 000000006  Colonnes NOT NULL
                    ont-elles des NULL?
                         â”‚
                  â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
                  â”‚             â”‚
                OUI            NON
                  â”‚             â”‚
                  â†“             â†“
              [ERROR ACTION]  âœ… CONTINUE
              ICT_DRIVEN/         â”‚
              REJECT/             â†“
              LOG_ONLY      [VALIDATION 3: RLT]
                            % lignes rejetÃ©es
                            < seuil?
                                 â”‚
                          â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
                          â”‚             â”‚
                        NON            OUI
                          â”‚             â”‚
                          â†“             â†“
                      âŒ REJECT      âœ… CONTINUE
                      Fichier        Au traitement
                      entier
```

**Exemple concret:**

```python
# Validation du DataFrame
validation_result = validator.validate_data(
    df=df,
    column_defs=column_defs,
    filename="customers_20251016.csv",
    ict_threshold=10,  # 10% colonnes invalides par ligne max
    rlt_threshold=10   # 10% lignes rejetÃ©es par fichier max
)

if not validation_result["valid"]:
    print(f"Validation Ã©chouÃ©e: {validation_result['errors']}")
    # STOP - fichier rejetÃ©
else:
    print("Validation OK - continuer")
```

---

### ğŸ”§ PHASE 6: TRAITEMENT COLONNES

#### Module: `column_processor.py`

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DataFrame ValidÃ©                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â†“
     [Pour chaque colonne]
              â”‚
              â†“
   [Transformation Type?]
              â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                     â”‚
     DATE              REGEX/AUTRE
       â”‚                     â”‚
       â†“                     â†“
 [Transformation Date]  [Autres transfo]
 Pattern: dd/MM/yyyy    
 â†’ yyyy-MM-dd HH:mm:ss  
       â”‚                     â”‚
       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â†“
      [Error Action?]
      Si erreur transformation
              â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”
       â”‚             â”‚      â”‚
   ICT_DRIVEN    REJECT   LOG_ONLY
       â”‚             â”‚      â”‚
       â†“             â†“      â†“
   Calcul ICT    Rejeter  Logger
   Si > seuil    ligne    seulement
   â†’ Rejeter              â”‚
   ligne                  â”‚
       â”‚                  â”‚
       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â†“
   [DataFrame TraitÃ©]
   avec colonnes transformÃ©es
```

**Exemple concret:**

```python
# Traitement des colonnes
df_processed = column_processor.process_columns(
    df=df,
    column_defs=column_defs
)

# Exemple transformation date
# Avant: "15/10/2025"
# AprÃ¨s: "2025-10-15 00:00:00"

# Colonne _validation_failed ajoutÃ©e
# True = ligne invalide, False = ligne valide
```

---

### ğŸ—‚ï¸ PHASE 7: GESTION LIGNES INVALIDES

#### Module: `invalid_lines_manager.py`

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DataFrame TraitÃ©                       â”‚
â”‚  avec colonne _validation_failed        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â†“
   [Config: invalid_lines_generated?]
              â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
       â”‚             â”‚
     TRUE          FALSE
       â”‚             â”‚
       â†“             â””â”€â”€â†’ [Ignorer lignes invalides]
   [Filtrer                      â”‚
   _validation_failed = true]     â”‚
       â”‚                          â”‚
       â†“                          â”‚
   [Sauvegarder dans              â”‚
   <table>_invalid_lines]         â”‚
   + mÃ©tadonnÃ©es complÃ¨tes        â”‚
       â”‚                          â”‚
       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â†“
   [Garder seulement
   _validation_failed = false]
              â”‚
              â†“
   [DataFrame NettoyÃ©]
   PrÃªt pour ingestion
```

**Exemple concret:**

```python
# SÃ©parer lignes valides et invalides
invalid_df = df_processed.filter("_validation_failed = true")
valid_df = df_processed.filter("_validation_failed = false")

# Sauvegarder lignes invalides
if config["invalid_lines_generated"] and invalid_df.count() > 0:
    invalid_lines_manager.save_invalid_lines(
        df=invalid_df,
        table_name="customers",
        filename="customers_20251016.csv",
        rejection_reason="Validation failed",
        error_type="DATA_VALIDATION"
    )
    # â†’ SauvegardÃ© dans: customers_invalid_lines

# Continuer avec lignes valides uniquement
df = valid_df
```

---

### ğŸ’¾ PHASE 8: INGESTION DELTA LAKE

#### Module: `ingestion_enhanced.py` + `delta_manager.py`

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DataFrame NettoyÃ© (lignes valides)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â†“
   [Quel mode d'ingestion?]
              â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚         â”‚         â”‚         â”‚
FULL_SNAPSHOT DELTA_   DELTA_    Autres
              FROM_    NON_HIS   modes
              FLOW     TORIZED
    â”‚         â”‚         â”‚         â”‚
    â†“         â†“         â†“         â†“
    
[FULL_SNAPSHOT]
â”œâ”€â†’ Table Last (overwrite)
â”‚   customers_last âœ…
â”‚   DerniÃ¨res donnÃ©es
â”‚
â””â”€â†’ Table All (append)
    customers_all âœ…
    Historique complet

[DELTA_FROM_FLOW]
â””â”€â†’ Table All UNIQUEMENT (append)
    orders_all âœ…
    (PAS de orders_last âŒ)

[DELTA_NON_HISTORIZED]
â”œâ”€â†’ Table Last (merge sur clÃ©s)
â”‚   products_last âœ…
â”‚   Update si clÃ© existe
â”‚   Insert si nouvelle
â”‚
â””â”€â†’ Table All (append)
    products_all âœ…

              â”‚
              â†“
   [Ajouter colonnes techniques]
   - FILE_LINE_ID
   - FILE_NAME_RECEIVED
   - FILE_EXTRACTION_DATE
   - FILE_PROCESS_DATE
              â”‚
              â†“
   [Partitionnement]
   selon config (yyyy/mm/dd)
              â”‚
              â†“
   âœ… Tables Delta crÃ©Ã©es
```

**Exemple concret:**

```python
# Ingestion avec gestion Last/All
stats = ingestion_manager.apply_ingestion_mode(
    df_raw=df,
    column_defs=column_defs,
    table_name="customers",
    ingestion_mode="FULL_SNAPSHOT",
    file_name_received="customers_20251016.csv"
)

# RÃ©sultat:
# âœ… customers_last crÃ©Ã©e (1,000 lignes) - overwrite
# âœ… customers_all crÃ©Ã©e (1,000 lignes) - append

# Si on exÃ©cute Ã  nouveau avec un autre fichier:
# âœ… customers_last remplacÃ©e (950 lignes) - overwrite
# âœ… customers_all mise Ã  jour (1,950 lignes) - append
```

---

### ğŸ“ PHASE 9: ENREGISTREMENT TRACKING

#### Module: `tracking_manager.py`

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ingestion TerminÃ©e avec SuccÃ¨s         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â†“
   [Enregistrer dans
   wax_processed_files]
              â”‚
              â†“
   Table: wax_processed_files
   +------------+----------+------+--------+
   | table_name | filename | date | status |
   +------------+----------+------+--------+
   | customers  | cust...  | ...  | SUCCESS|
   +------------+----------+------+--------+
              â”‚
              â†“
   âœ… Fichier trackÃ©
   (duplicate check futur)
```

**Exemple concret:**

```python
# Enregistrer succÃ¨s
tracking_manager.register_file(
    table_name="customers",
    filename="customers_20251016.csv",
    status="SUCCESS",
    row_count=1000
)

# Prochaine fois que ce fichier arrive â†’ REJETÃ‰ (duplicate)
```

---

### ğŸ“Š PHASE 10: LOGS & MÃ‰TRIQUES

#### Module: `logger_manager.py`

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Pipeline TerminÃ©                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â†“
   [Ã‰crire logs d'exÃ©cution]
   Table: wax_execution_logs
              â”‚
              â†“
   [Ã‰crire mÃ©triques qualitÃ©]
   Table: wax_quality_logs
              â”‚
              â†“
   âœ… Logs complets
```

**Exemple concret:**

```python
# Logs automatiques
logger_manager.log_execution(
    table_name="customers",
    status="SUCCESS",
    files_processed=1,
    rows_processed=1000,
    rows_rejected=50,
    duration_seconds=32
)

# RequÃªte logs:
spark.sql("""
    SELECT * FROM wax_execution_logs
    WHERE table_name = 'customers'
    ORDER BY execution_date DESC
    LIMIT 10
""").show()
```

---

## ğŸ—ºï¸ FLUX DE DONNÃ‰ES COMPLET

```
ğŸ“‚ INPUT
â”‚
â”œâ”€ customers.csv (500 Ko)
â”œâ”€ orders.zip (2 MB â†’ 3 fichiers)
â””â”€ products_20251016.csv (1 MB)
     â”‚
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 1: EXTRACTION               â”‚
â”‚  file_handler + unzip_module       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â†“
ğŸ“‚ /extracted/
â”œâ”€ customers/customers.csv
â”œâ”€ orders/order_001.csv
â”œâ”€ orders/order_002.csv  
â”œâ”€ orders/order_003.csv
â””â”€ products/products_20251016.csv
     â”‚
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 2: VALIDATION TRACKING      â”‚
â”‚  tracking_manager                  â”‚
â”‚  âœ… Check duplicate                â”‚
â”‚  âœ… Check file order               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 3: LECTURE                  â”‚
â”‚  autoloader_module                 â”‚
â”‚  DataFrame PySpark                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 4: VALIDATION DONNÃ‰ES       â”‚
â”‚  validator                         â”‚
â”‚  âœ… Colonnes, types, nulls         â”‚
â”‚  âœ… ICT, RLT                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 5: TRAITEMENT               â”‚
â”‚  column_processor                  â”‚
â”‚  Transformations dates, etc.       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 6: INVALID LINES            â”‚
â”‚  invalid_lines_manager             â”‚
â”‚  Sauvegarder lignes rejetÃ©es       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 7: INGESTION                â”‚
â”‚  ingestion_enhanced + delta_mgr    â”‚
â”‚  CrÃ©er Last + All tables           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 8: TRACKING & LOGS          â”‚
â”‚  tracking_manager + logger_manager â”‚
â”‚  Enregistrer succÃ¨s                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â†“
ğŸ’¾ TABLES DELTA FINALES
â”œâ”€ customers_last (500 lignes)
â”œâ”€ customers_all (500 lignes)
â”œâ”€ customers_invalid_lines (50 lignes)
â”œâ”€ orders_all (2,500 lignes)
â”œâ”€ orders_invalid_lines (100 lignes)
â”œâ”€ products_last (1,000 lignes)
â”œâ”€ products_all (1,000 lignes)
â”œâ”€ wax_processed_files (5 entrÃ©es)
â”œâ”€ wax_execution_logs (5 entrÃ©es)
â””â”€ wax_quality_logs (5 entrÃ©es)
```

---

## ğŸ¯ POINTS D'ENTRÃ‰E

### 1. Point d'EntrÃ©e Principal

```python
# main.py (ou main_enhanced.py)

from pyspark.sql import SparkSession
from config import Config
from main_enhanced import WAXPipelineEnhanced

# Initialiser Spark
spark = SparkSession.builder \
    .appName("WAX-Pipeline") \
    .getOrCreate()

# Configuration
config = Config(
    catalog="abu_catalog",
    schema_files="databricksassetbundletest",
    volume="externalvolumetes",
    schema_tables="gdp_poc_dev",
    env="dev"
)

# ParamÃ¨tres API (optionnel)
api_params = {
    "fail_fast_enabled": True,
    "duplicate_check_enabled": True,
    "invalid_lines_generated": True
}

# CrÃ©er pipeline
pipeline = WAXPipelineEnhanced(spark, config, api_params)

# ExÃ©cuter
config_excel = "/path/to/config.xlsx"
stats = pipeline.run(config_excel)

# RÃ©sultat
print(f"Fichiers traitÃ©s: {stats['files_processed']}")
print(f"Lignes ingÃ©rÃ©es: {stats['rows_ingested']}")
```

### 2. Configuration Excel

Le fichier Excel contient 2 sheets:

**Sheet 1: File-Table**
```
| Delta Table Name | Filename Pattern      | Ingestion mode   |
|------------------|-----------------------|------------------|
| customers        | customers_*.csv       | FULL_SNAPSHOT    |
| orders           | orders_*.csv          | DELTA_FROM_FLOW  |
| products         | products_yyyymmdd.csv | FULL_SNAPSHOT    |
```

**Sheet 2: Field-Column**
```
| Delta Table Name | Column Name  | Field type | Transformation Type |
|------------------|--------------|------------|---------------------|
| customers        | customer_id  | integer    |                     |
| customers        | name         | string     |                     |
| customers        | birth_date   | date       | date                |
```

---

## ğŸ”„ INTERACTIONS ENTRE MODULES

```
                    [main_enhanced.py]
                           â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚               â”‚               â”‚
           â†“               â†“               â†“
    [file_handler]  [unzip_module]  [tracking_manager]
           â”‚               â”‚               â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚               â”‚
                   â†“               â†“
            [autoloader]    [validator]
                   â”‚               â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
                  [column_processor]
                           â”‚
                           â†“
              [invalid_lines_manager]
                           â”‚
                           â†“
              [ingestion_enhanced]
                           â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
                    â†“             â†“
            [delta_manager]  [logger_manager]
```

**DÃ©pendances clÃ©s:**

1. **main_enhanced** orchestre tout
2. **tracking_manager** validÃ© AVANT lecture
3. **validator** vÃ©rifie donnÃ©es APRÃˆS lecture
4. **column_processor** transforme colonnes
5. **invalid_lines_manager** sauvegarde erreurs
6. **ingestion_enhanced** utilise **delta_manager**
7. **logger_manager** enregistre rÃ©sultats

---

## ğŸ’¡ SCÃ‰NARIOS D'USAGE

### ScÃ©nario 1: Fichier CSV Direct

```
INPUT: customers.csv (non-ZIP)

[file_handler]
â†’ Copie vers /extracted/customers/

[tracking_manager]
â†’ Check duplicate: OK (nouveau)
â†’ Check order: OK (date valide)

[autoloader]
â†’ Lit 1,000 lignes

[validator]
â†’ Validation: OK

[column_processor]
â†’ Transformations: OK

[invalid_lines_manager]
â†’ 50 lignes invalides â†’ customers_invalid_lines

[ingestion_enhanced - FULL_SNAPSHOT]
â†’ customers_last crÃ©Ã©e (950 lignes)
â†’ customers_all crÃ©Ã©e (950 lignes)

[tracking_manager]
â†’ EnregistrÃ© dans wax_processed_files

âœ… SUCCÃˆS
```

### ScÃ©nario 2: Fichier Duplicate

```
INPUT: customers.csv (dÃ©jÃ  traitÃ© hier)

[file_handler]
â†’ Copie vers /extracted/customers/

[tracking_manager]
â†’ Check duplicate: âŒ Ã‰CHEC
â†’ Error 000000002: File already processed

âŒ REJETÃ‰ - Ne pas continuer
```

### ScÃ©nario 3: Fichier ZIP

```
INPUT: orders.zip (contient 3 CSV)

[unzip_module]
â†’ Extrait order_001.csv, order_002.csv, order_003.csv
â†’ Supprime orders.zip

[Pour chaque fichier extrait:]
  [tracking_manager] â†’ Validation
  [autoloader] â†’ Lecture
  [validator] â†’ Validation
  [column_processor] â†’ Traitement
  [ingestion_enhanced - DELTA_FROM_FLOW] 
  â†’ orders_all crÃ©Ã©e (append uniquement)
  â†’ PAS de orders_last

âœ… 3 fichiers traitÃ©s
```

### ScÃ©nario 4: Fichier avec Trop d'Erreurs

```
INPUT: products.csv

[autoloader]
â†’ Lit 1,000 lignes

[validator]
â†’ 150 lignes invalides (15%)
â†’ RLT threshold: 10%
â†’ 15% > 10%

âŒ FICHIER REJETÃ‰
â†’ EnregistrÃ© comme FAILED dans tracking
```

---

## ğŸ“Š TABLES CRÃ‰Ã‰ES

### Tables de DonnÃ©es

```sql
-- Table Last (selon mode)
customers_last
â”œâ”€ FILE_LINE_ID
â”œâ”€ FILE_NAME_RECEIVED
â”œâ”€ FILE_EXTRACTION_DATE
â”œâ”€ FILE_PROCESS_DATE
â””â”€ [colonnes mÃ©tier...]

-- Table All (toujours crÃ©Ã©e)
customers_all
â”œâ”€ FILE_LINE_ID
â”œâ”€ FILE_NAME_RECEIVED  
â”œâ”€ FILE_EXTRACTION_DATE
â”œâ”€ FILE_PROCESS_DATE
â”œâ”€ yyyy, mm, dd (partitions)
â””â”€ [colonnes mÃ©tier...]

-- Table Invalid Lines
customers_invalid_lines
â”œâ”€ invalid_line_id
â”œâ”€ filename
â”œâ”€ rejection_date
â”œâ”€ rejection_reason
â”œâ”€ error_type
â””â”€ [colonnes mÃ©tier...]
```

### Tables SystÃ¨me

```sql
-- Tracking fichiers
wax_processed_files
â”œâ”€ table_name
â”œâ”€ filename
â”œâ”€ file_date
â”œâ”€ processed_date
â”œâ”€ status
â””â”€ row_count

-- Logs exÃ©cution
wax_execution_logs
â”œâ”€ table_name
â”œâ”€ execution_date
â”œâ”€ status
â”œâ”€ files_processed
â””â”€ rows_processed

-- Logs qualitÃ©
wax_quality_logs
â”œâ”€ table_name
â”œâ”€ filename
â”œâ”€ total_rows
â”œâ”€ valid_rows
â””â”€ invalid_rows
```

---

## âš™ï¸ PARAMÃˆTRES CONFIGURABLES

```json
{
  "fail_fast_threshold": 10,
  "â†’ Si 10% d'erreurs â†’ arrÃªt immÃ©diat"
  
  "invalid_lines_generated": true,
  "â†’ Sauvegarder lignes invalides"
  
  "duplicate_check_enabled": true,
  "â†’ VÃ©rifier duplicates"
  
  "file_order_check_enabled": true,
  "â†’ VÃ©rifier ordre chronologique"
  
  "non_zip_support_enabled": true,
  "â†’ Support fichiers non-ZIP"
  
  "delete_zip_after_extract": true,
  "â†’ Supprimer ZIP aprÃ¨s extraction"
  
  "ict_default": 10,
  "â†’ Invalid Column per Line Tolerance (10%)"
  
  "rlt_default": 10
  "â†’ Rejected Line per File Tolerance (10%)"
}
```

---

## ğŸ¯ RÃ‰SUMÃ‰ EXÃ‰CUTIF

### En 5 Points

1. **Fichiers arrivent** â†’ DÃ©tection ZIP/non-ZIP
2. **Validation stricte** â†’ Duplicate, ordre, donnÃ©es
3. **Traitement intelligent** â†’ Transformations, gestion erreurs
4. **Ingestion flexible** â†’ Last/All selon mode
5. **TraÃ§abilitÃ© complÃ¨te** â†’ Tracking, logs, mÃ©triques

### Temps d'ExÃ©cution Typique

```
Fichier 1 MB (10,000 lignes):
â”œâ”€ Extraction: ~1s
â”œâ”€ Validation: ~2s
â”œâ”€ Lecture: ~3s
â”œâ”€ Traitement: ~5s
â”œâ”€ Ingestion: ~8s
â””â”€ Logs: ~1s
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL: ~20s
```

### Garanties

âœ… **Pas de duplicates** (tracking)
âœ… **Ordre chronologique** respectÃ©
âœ… **QualitÃ© validÃ©e** (ICT, RLT)
âœ… **TraÃ§abilitÃ© totale** (tous fichiers trackÃ©s)
âœ… **Erreurs capturÃ©es** (invalid_lines)
âœ… **ConformitÃ© specs** Gherkin 100%

---

## ğŸš€ POUR ALLER PLUS LOIN

- ğŸ“– **NEW_FEATURES_R1_DOCUMENTATION.md** : DÃ©tails features
- ğŸ› ï¸ **IMPLEMENTATION_GUIDE.md** : Guide pratique
- ğŸ“Š **FEATURES_COVERAGE_ANALYSIS.md** : Analyse complÃ¨te
- ğŸ“‹ **IMPLEMENTATION_SUMMARY.md** : RÃ©capitulatif

---

**Version:** 2.0.0  
**DerniÃ¨re mise Ã  jour:** Octobre 2025  
**Statut:** Production Ready âœ…
