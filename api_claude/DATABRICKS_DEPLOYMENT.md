# üöÄ D√©ploiement API Configuration sur Databricks

## üìã Vue d'ensemble

Ce guide explique comment d√©ployer l'API de configuration sur Databricks pour rendre votre pipeline WAX portable et param√©trable.

---

## üìÅ Structure des Fichiers

```
/Volumes/abu_catalog/databricksassetbundletest/externalvolumetes/
‚îú‚îÄ‚îÄ input/
‚îÇ   ‚îî‚îÄ‚îÄ config/
‚îÇ       ‚îú‚îÄ‚îÄ config_params.json      # Param√®tres de configuration
‚îÇ       ‚îî‚îÄ‚îÄ wax_config.xlsx         # Config Excel existante
‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îú‚îÄ‚îÄ config_api.py               # API Flask
‚îÇ   ‚îú‚îÄ‚îÄ config_client.py            # Client Python
‚îÇ   ‚îî‚îÄ‚îÄ start_api.py                # Script de d√©marrage
‚îî‚îÄ‚îÄ code/
    ‚îú‚îÄ‚îÄ config.py                   # Config modifi√©e (utilise API)
    ‚îî‚îÄ‚îÄ ... (autres modules WAX)
```

---

## üîß √âTAPE 1 : Uploader les Fichiers

### Option A : Via UI Databricks

1. Aller dans **Workspace** ‚Üí Votre r√©pertoire de projet
2. Cr√©er un r√©pertoire `api/`
3. Uploader les fichiers :
   - `config_api.py`
   - `config_client.py`
   - `start_api.py`
   - `config_params.json`

### Option B : Via Databricks CLI

```bash
# Installer Databricks CLI
pip install databricks-cli

# Configurer
databricks configure --token

# Uploader
databricks workspace import config_api.py /Workspace/Users/you@company.com/wax/api/config_api.py
databricks workspace import config_client.py /Workspace/Users/you@company.com/wax/api/config_client.py
databricks workspace import config_params.json /Volumes/abu_catalog/databricksassetbundletest/externalvolumetes/input/config/config_params.json
```

### Option C : Via dbutils (dans un notebook)

```python
# Cr√©er le fichier config_params.json dans le volume
import json

config_data = {
    "dev": {
        "catalog": "abu_catalog",
        "schema_files": "databricksassetbundletest",
        "volume": "externalvolumetes",
        "schema_tables": "gdp_poc_dev",
        "env": "dev",
        "version": "v1"
    },
    "int": { ... },
    "prd": { ... }
}

config_path = "/Volumes/abu_catalog/databricksassetbundletest/externalvolumetes/input/config/config_params.json"
dbutils.fs.put(config_path, json.dumps(config_data, indent=2), overwrite=True)
```

---

## üåê √âTAPE 2 : D√©marrer l'API

### Option 1 : Via Notebook Databricks (Recommand√© pour tests)

Cr√©er un notebook `Start_Config_API.py` :

```python
# Cellule 1 : Installer Flask
%pip install flask requests

# Cellule 2 : Charger l'API
import sys
sys.path.append("/Workspace/Users/you@company.com/wax/api")

from config_api import app
import os

# Configurer
os.environ['CONFIG_FILE_PATH'] = '/Volumes/abu_catalog/databricksassetbundletest/externalvolumetes/input/config/config_params.json'

# D√©marrer (mode test)
print("‚úÖ API pr√™te - Utiliser les fonctions directement ou via requests")
```

**Pour tester dans le notebook :**

```python
# Cellule 3 : Tester l'API
from config_client import ConfigClient

client = ConfigClient(api_url="http://localhost:5000")

# Health check
health = client.health_check()
print(health)

# R√©cup√©rer config dev
config = client.get_config("dev")
print(config)
```

### Option 2 : Via Databricks Job (Production)

**Cr√©er un Job Databricks :**

1. **Jobs** ‚Üí **Create Job**
2. **Task name** : `Config_API_Server`
3. **Type** : Python script
4. **Script** : `/Workspace/Users/you@company.com/wax/api/start_api.py`
5. **Cluster** : Cluster existant ou nouveau (Standard_DS3_v2 suffit)
6. **Schedule** : Always running (ou on-demand)

**Script start_api.py pour Databricks :**

```python
import os
import sys
from flask import Flask

# Configuration
os.environ['CONFIG_FILE_PATH'] = '/dbfs/Volumes/abu_catalog/databricksassetbundletest/externalvolumetes/input/config/config_params.json'
os.environ['API_PORT'] = '8080'

# Importer l'API
sys.path.append("/dbfs/Workspace/Users/you@company.com/wax/api")
from config_api import app

if __name__ == '__main__':
    print("üöÄ API Configuration WAX - Databricks")
    
    # D√©marrer sur le port du cluster
    app.run(
        host='0.0.0.0',
        port=8080,
        debug=False  # Production mode
    )
```

### Option 3 : Via Databricks Serving (Serverless - Avanc√©)

Pour une solution serverless, utiliser **Databricks Model Serving** :

```python
# Cr√©er un endpoint serverless
from databricks.sdk import WorkspaceClient

w = WorkspaceClient()

w.serving_endpoints.create(
    name="wax-config-api",
    config={
        "served_models": [{
            "model_name": "config_api",
            "model_version": "1",
            "workload_size": "Small",
            "scale_to_zero_enabled": True
        }]
    }
)
```

---

## üîó √âTAPE 3 : Obtenir l'URL de l'API

### Si API via Notebook/Job

L'URL d√©pend du cluster :

```
http://<cluster-id>.<region>.azuredatabricks.net:8080
```

**Trouver l'URL :**

1. Cluster ‚Üí Configuration
2. Noter le **Cluster ID** et la **Region**
3. URL compl√®te : `http://<cluster-id>.<region>.azuredatabricks.net:8080`

### Si API via Databricks Serving

```
https://<workspace-url>/serving-endpoints/wax-config-api/invocations
```

---

## üß™ √âTAPE 4 : Tester avec Postman

### 1. Configurer l'authentification (si n√©cessaire)

**Databricks Token Auth :**

- Type : Bearer Token
- Token : Votre Databricks Personal Access Token

### 2. Tester les endpoints

```
GET http://<cluster-url>:8080/health
GET http://<cluster-url>:8080/config/dev
POST http://<cluster-url>:8080/config/test
```

**Headers :**
```
Authorization: Bearer dapi...
Content-Type: application/json
```

---

## üíª √âTAPE 5 : Utiliser dans votre Code

### Modifier vos scripts existants

**Avant (valeurs en dur) :**

```python
from config import Config

config = Config(
    catalog="abu_catalog",
    schema_files="databricksassetbundletest",
    volume="externalvolumetes",
    schema_tables="gdp_poc_dev",
    env="dev"
)
```

**Apr√®s (depuis API) :**

```python
from config import create_config_from_api
import os

# D√©finir l'URL de l'API
os.environ['CONFIG_API_URL'] = 'http://<cluster-url>:8080'

# Cr√©er config depuis API
config = create_config_from_api(env="dev")

# Utiliser normalement
config.print_config()
```

### Dans vos notebooks Databricks

```python
# Cellule 1 : Configuration
%pip install requests

# Cellule 2 : Charger config depuis API
import os
os.environ['CONFIG_API_URL'] = 'http://localhost:8080'  # Local au cluster

from config import create_config_from_api

# R√©cup√©rer config pour l'environnement souhait√©
config = create_config_from_api(env=dbutils.widgets.get("env"))

# Afficher
config.print_config()

# Cellule 3 : Lancer le pipeline
from main import main as waxng_ingestion

result = waxng_ingestion(config)
```

---

## üîê S√âCURIT√â

### Protection de l'API

**Option 1 : Databricks Network Security**

- Utiliser VPC priv√©
- Configurer Security Groups
- Limiter acc√®s aux IPs autoris√©es

**Option 2 : API Key Authentication**

Modifier `config_api.py` pour ajouter l'authentification :

```python
from functools import wraps
from flask import request, jsonify

API_KEY = os.getenv('CONFIG_API_KEY', 'your-secret-key')

def require_api_key(f):
    @wraps(f)
    def decorated(*args, **kwargs):
        key = request.headers.get('X-API-Key')
        if key != API_KEY:
            return jsonify({"error": "Invalid API key"}), 401
        return f(*args, **kwargs)
    return decorated

@app.route('/config/<env>', methods=['GET'])
@require_api_key
def get_config(env):
    # ...
```

---

## üìä MONITORING

### Logs de l'API

**Via Databricks Job Logs :**

1. Jobs ‚Üí Votre job API
2. Runs ‚Üí Latest run
3. Logs ‚Üí Driver logs

**Via Code :**

```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/dbfs/Volumes/.../logs/api.log'),
        logging.StreamHandler()
    ]
)
```

---

## üîÑ MISE √Ä JOUR DES CONFIGURATIONS

### Via Postman

```
POST http://<cluster-url>:8080/config/dev
Body: { "catalog": "new_value", ... }
```

### Via Code

```python
from config_client import ConfigClient

client = ConfigClient(api_url="http://<cluster-url>:8080")

client.create_or_update_config("dev", {
    "catalog": "abu_catalog_v2",
    "schema_files": "databricksassetbundletest_v2",
    "volume": "externalvolumetes_v2",
    "schema_tables": "gdp_poc_dev_v2",
    "env": "dev",
    "version": "v2"
})
```

### Via Notebook UI

Cr√©er un notebook avec widgets pour modifier la config :

```python
dbutils.widgets.text("env", "dev", "Environment")
dbutils.widgets.text("catalog", "", "Catalog")
dbutils.widgets.text("schema_tables", "", "Schema Tables")

# Bouton pour sauvegarder
env = dbutils.widgets.get("env")
catalog = dbutils.widgets.get("catalog")
schema_tables = dbutils.widgets.get("schema_tables")

# Mettre √† jour via API
from config_client import ConfigClient
client = ConfigClient()
client.create_or_update_config(env, {
    "catalog": catalog,
    "schema_tables": schema_tables,
    # ... autres champs
})
```

---

## ‚úÖ CHECKLIST D√âPLOIEMENT

- [ ] Fichiers upload√©s dans le volume Unity Catalog
- [ ] `config_params.json` cr√©√© et accessible
- [ ] API d√©marr√©e (Job ou Notebook)
- [ ] URL de l'API obtenue
- [ ] Tests Postman r√©ussis
- [ ] `config.py` modifi√© pour utiliser l'API
- [ ] Variable `CONFIG_API_URL` d√©finie
- [ ] Tests du pipeline avec la nouvelle config
- [ ] S√©curit√© configur√©e (API key ou network)
- [ ] Monitoring en place

---

## üêõ D√âPANNAGE

### Probl√®me : API ne d√©marre pas

**V√©rifier :**
- Flask install√© : `%pip install flask`
- Ports disponibles
- Logs du cluster

### Probl√®me : Config file not found

**V√©rifier :**
- Chemin du fichier : `/Volumes/...`
- Permissions sur le volume
- Variable `CONFIG_FILE_PATH`

### Probl√®me : Timeout connexion

**V√©rifier :**
- Cluster actif
- Port ouvert (8080)
- Firewall / Security Groups

---

## üìö RESSOURCES

- [Databricks Jobs](https://docs.databricks.com/jobs/)
- [Unity Catalog Volumes](https://docs.databricks.com/data-governance/unity-catalog/volumes.html)
- [Flask Documentation](https://flask.palletsprojects.com/)

---

‚úÖ **API pr√™te pour production !**
